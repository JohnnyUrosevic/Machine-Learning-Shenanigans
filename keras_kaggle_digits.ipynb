{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_kaggle_digits.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/JohnnyUrosevic/Machine-Learning-Shenanigans/blob/master/keras_kaggle_digits.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "62uEbpTwHJRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "8e31b3a1-3c16-4f0a-b36d-76dd18207fe0"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "!pip3 install hyperas\n",
        "!pip3 install networkx==1.11\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperas in /usr/local/lib/python3.6/dist-packages (0.4)\r\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.2.3)\r\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.1.6)\r\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.3.1)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.14.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (0.19.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.4.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.2)\n",
            "Requirement already satisfied: mistune>=0.7.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.3.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.3.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.11)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.7.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.3.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->nbconvert->hyperas) (1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->hyperas) (4.3.0)\n",
            "Requirement already satisfied: html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (1.0.1)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (5.2.3)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.3.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.15)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre->bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->hyperas) (2.5.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->hyperas) (16.0.4)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (4.6.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (39.1.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.7.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "Requirement already satisfied: networkx==1.11 in /usr/local/lib/python3.6/dist-packages (1.11)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from networkx==1.11) (4.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f5UUegXeBt8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download Files from google drive, and add this file to the temporary local system so it can be recognized by Hyperas"
      ]
    },
    {
      "metadata": {
        "id": "eyvT0Of9HNtp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e4a1a18c-f98c-4a0b-e535-3f7c7a88dc82"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import io , requests, os\n",
        "import sys\n",
        "auth.authenticate_user()\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "SOURCE_FOLDER='/content/datalab/'\n",
        "\n",
        "\n",
        "def get_parent_folder(folder_name):\n",
        "  page_token = None\n",
        "  folder_array = []\n",
        "  query = \"name='%s' and mimeType='application/vnd.google-apps.folder'\" % folder_name\n",
        "  while True:\n",
        "      response = drive_service.files().list(q=query,\n",
        "                                          spaces='drive',\n",
        "                                          fields='nextPageToken, files(id, name)',\n",
        "                                          pageToken=page_token).execute()\n",
        "      for file in response.get('files', []):\n",
        "          # Process change\n",
        "          #print (file.get('name'), file.get('id'))\n",
        "          folder_array.append({\"name\" : file.get('name'), \"id\" : file.get('id')})\n",
        "      page_token = response.get('nextPageToken', None)\n",
        "      if page_token is None:\n",
        "          break\n",
        "  return folder_array\n",
        "\n",
        "\n",
        "def get_files_from_parent(parent_id):\n",
        "  page_token = None\n",
        "  folder_array = dict()\n",
        "  query = \"'%s' in parents\" % parent_id\n",
        "  while True:\n",
        "      response = drive_service.files().list(q=query,\n",
        "                                          spaces='drive',\n",
        "                                          fields='nextPageToken, files(id, name)',\n",
        "                                          pageToken=page_token).execute()\n",
        "      for file in response.get('files', []):\n",
        "          # Process change\n",
        "          #print (file.get('name'), file.get('id'))\n",
        "          folder_array.update({file.get('name'):file.get('id')})\n",
        "      page_token = response.get('nextPageToken', None)\n",
        "      if page_token is None:\n",
        "          break\n",
        "  return folder_array\n",
        "\n",
        "def get_file_buffer(file_id, verbose=0):\n",
        "  from googleapiclient.http import MediaIoBaseDownload\n",
        "  request = drive_service.files().get_media(fileId=file_id)\n",
        "  downloaded = io.BytesIO()\n",
        "  downloader = MediaIoBaseDownload(downloaded, request)\n",
        "  done = False\n",
        "  while done is False:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "    progress, done = downloader.next_chunk()\n",
        "    if verbose:\n",
        "      sys.stdout.flush()\n",
        "      sys.stdout.write('\\r')\n",
        "      percentage_done = progress.resumable_progress * 100/progress.total_size\n",
        "      sys.stdout.write(\"[%-100s] %d%%\" % ('='*int(percentage_done), int(percentage_done)))\n",
        "  downloaded.seek(0)\n",
        "  return downloaded\n",
        "\n",
        "parent_folder = get_parent_folder('Kaggle Digit')\n",
        "\n",
        "input_file_meta = get_files_from_parent(parent_folder[0][\"id\"])\n",
        "\n",
        "\n",
        "for file, id in input_file_meta.items():\n",
        "  downloaded = get_file_buffer(id, verbose=1)\n",
        "  dest_file = os.path.join(SOURCE_FOLDER, file)\n",
        "  print(\"processing %s data\" % file)\n",
        "  with open(dest_file, \"wb\") as out:\n",
        "    out.write(downloaded.read())\n",
        "    print(\"Done %s\" % dest_file)\n",
        "\n",
        "!pip3 install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "    \n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Copy/download this file, Is this legal?\n",
        "fid = drive.ListFile({'q':\"title='keras_kaggle_digits.ipynb'\"}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile('keras_kaggle_digits.ipynb')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[====================================================================================================] 100%processing test.csv data\n",
            "Done /content/datalab/test.csv\n",
            "[====================================================================================================] 100%processing train.csv data\n",
            "Done /content/datalab/train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q7b9WfbWHVoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define Model\n",
        "def create_model(x_train, y_train, x_test, y_test):\n",
        "    \n",
        "    \n",
        "    #input layer\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    #convolution layer 1\n",
        "    model.add(keras.layers.Conv2D({{choice([32, 40, 50, 64])}}, {{choice([(3,3),(5,5)])}}, input_shape=(28, 28, 1), activation={{choice([tf.nn.elu, tf.nn.relu, tf.nn.leaky_relu])}}))\n",
        "\n",
        "    #pool layer 1\n",
        "    #input [batch_size, 24, 24, 20]\n",
        "    #output [batch_size, 12, 12, 20]\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    #convolution layer 2\n",
        "    model.add(keras.layers.Conv2D({{choice([32, 40, 50, 64])}}, {{choice([(3,3),(5,5)])}}, activation={{choice([tf.nn.elu, tf.nn.relu, tf.nn.leaky_relu])}}))\n",
        "   \n",
        "    \n",
        "    #pool layer 2\n",
        "    #input [batch_size, 8, 8, 40]\n",
        "    #output [batch_size, 4, 4, 40]\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    #pool layer 3\n",
        "    if {{choice(['add','dont'])}} == 'add':\n",
        "\n",
        "        #convolution layer 2\n",
        "        model.add(keras.layers.Conv2D({{choice([32, 40, 50, 64])}}, (3,3), activation={{choice([tf.nn.elu, tf.nn.relu, tf.nn.leaky_relu])}}))\n",
        "        model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(keras.layers.Flatten())\n",
        "    \n",
        "    #dense layer 1\n",
        "    model.add(keras.layers.Dense({{choice([50, 100, 150, 200])}}, activation={{choice([tf.nn.elu, tf.nn.relu, tf.nn.leaky_relu])}}))\n",
        "    \n",
        "    #dropout layer\n",
        "    model.add(keras.layers.Dropout({{uniform(0,1)}}))\n",
        "    \n",
        "    #dense layer 2\n",
        "    model.add(keras.layers.Dense({{choice([50, 100, 150, 200])}}, activation={{choice([tf.nn.elu, tf.nn.relu, tf.nn.leaky_relu])}}))\n",
        "    \n",
        "    #dropout layer\n",
        "    model.add(keras.layers.Dropout({{uniform(0,1)}}))\n",
        "    \n",
        "    #output layer\n",
        "    model.add(keras.layers.Dense(10, activation=tf.nn.softmax))       \n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
        "                  optimizer={{choice(['rmsprop', 'adam', 'adadelta', 'sgd', 'RMSprop'])}})\n",
        "\n",
        "    \n",
        "    train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=30,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        fill_mode='constant',\n",
        "        cval=0,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "    batch_size = 1\n",
        "    \n",
        "    if {{choice([64, 128])}} == 64:\n",
        "        batch_size = 64\n",
        "    else:\n",
        "        batch_size = 128\n",
        "    \n",
        "    train_generator = train_datagen.flow(\n",
        "        x_train, y_train,  \n",
        "        batch_size=batch_size\n",
        "    ) \n",
        "\n",
        "\n",
        "    \n",
        "    model.fit_generator(train_generator,\n",
        "              steps_per_epoch=42000 // batch_size,\n",
        "              epochs=1,\n",
        "              verbose=2)\n",
        "    score, acc = model.evaluate(x_test / 255, y_test, verbose=0)\n",
        "    print('Test accuracy:', acc)\n",
        "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xlw-t7_eX4Lt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get input\n",
        "def data():\n",
        "    import numpy as np\n",
        "    data = np.loadtxt(\"/content/datalab/train.csv\", skiprows=1, delimiter=',')\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    features = np.reshape(data[:, 1:], (-1, 28, 28))\n",
        "\n",
        "    features = features.astype('float32')\n",
        "\n",
        "    labels = np.reshape(data[:, 0], (-1))\n",
        "    labels = labels.astype(np.int32)\n",
        "    expanded_labels = labels\n",
        "\n",
        "    \n",
        "    expanded_features = features.reshape(-1, 28, 28, 1)\n",
        "    \n",
        "    num_elements = expanded_features.shape[0]\n",
        "    x_train = expanded_features[:int(num_elements * .95)]\n",
        "    x_test = expanded_features[int(num_elements * .95):]\n",
        "    y_train = expanded_labels[:int(num_elements * .95)]\n",
        "    y_test = expanded_labels[int(num_elements* .95):]\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = keras.utils.to_categorical(y_test, 10)\n",
        "    \n",
        "    return x_train, y_train, x_test, y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8E_ee_KxMJuk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ac14c2b-d1be-434a-d082-837bc38c368d"
      },
      "cell_type": "code",
      "source": [
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "\n",
        "\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "\n",
        "#Get input\n",
        "test_data = np.loadtxt(\"/content/datalab/test.csv\", skiprows=1, delimiter=',')\n",
        "test_features = np.reshape(test_data, (-1, 28, 28, 1))\n",
        "                          "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ocAlFfDpGps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5049
        },
        "outputId": "0ea2ff46-2a58-4467-e5f3-4a68854abc26"
      },
      "cell_type": "code",
      "source": [
        "best_run, best_model = optim.minimize(model=create_model,\n",
        "                                          data=data,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          max_evals=20,\n",
        "                                          trials=Trials(),\n",
        "                                          notebook_name='keras_kaggle_digits')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    import tensorflow as tf\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow import keras\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import files\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import auth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from googleapiclient.discovery import build\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import io, requests, os\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import sys\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from googleapiclient.discovery import build\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from googleapiclient.http import MediaIoBaseDownload\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.auth import GoogleAuth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.drive import GoogleDrive\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import auth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from oauth2client.client import GoogleCredentials\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import files\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'Conv2D': hp.choice('Conv2D', [32, 40, 50, 64]),\n",
            "        'Conv2D_1': hp.choice('Conv2D_1', [(3,3),(5,5)]),\n",
            "        'activation': hp.choice('activation', [tf.nn.elu, tf.nn.relu, tf.nn.leaky_relu]),\n",
            "        'Conv2D_2': hp.choice('Conv2D_2', [32, 40, 50, 64]),\n",
            "        'Conv2D_3': hp.choice('Conv2D_3', [(3,3),(5,5)]),\n",
            "        'activation_1': hp.choice('activation_1', [tf.nn.elu, tf.nn.relu, tf.nn.leaky_relu]),\n",
            "        'activation_2': hp.choice('activation_2', ['add','dont']),\n",
            "        'Conv2D_4': hp.choice('Conv2D_4', [32, 40, 50, 64]),\n",
            "        'activation_3': hp.choice('activation_3', [tf.nn.elu, tf.nn.relu, tf.nn.leaky_relu]),\n",
            "        'Dense': hp.choice('Dense', [50, 100, 150, 200]),\n",
            "        'activation_4': hp.choice('activation_4', [tf.nn.elu, tf.nn.relu, tf.nn.leaky_relu]),\n",
            "        'Dropout': hp.uniform('Dropout', 0,1),\n",
            "        'Dense_1': hp.choice('Dense_1', [50, 100, 150, 200]),\n",
            "        'activation_5': hp.choice('activation_5', [tf.nn.elu, tf.nn.relu, tf.nn.leaky_relu]),\n",
            "        'Dropout_1': hp.uniform('Dropout_1', 0,1),\n",
            "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'adadelta', 'sgd', 'RMSprop']),\n",
            "        'optimizer_1': hp.choice('optimizer_1', [64, 128]),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "  1: \n",
            "  2: import numpy as np\n",
            "  3: data = np.loadtxt(\"/content/datalab/train.csv\", skiprows=1, delimiter=',')\n",
            "  4: np.random.shuffle(data)\n",
            "  5: \n",
            "  6: features = np.reshape(data[:, 1:], (-1, 28, 28))\n",
            "  7: \n",
            "  8: features = features.astype('float32')\n",
            "  9: \n",
            " 10: labels = np.reshape(data[:, 0], (-1))\n",
            " 11: labels = labels.astype(np.int32)\n",
            " 12: expanded_labels = labels\n",
            " 13: \n",
            " 14: \n",
            " 15: expanded_features = features.reshape(-1, 28, 28, 1)\n",
            " 16: \n",
            " 17: num_elements = expanded_features.shape[0]\n",
            " 18: x_train = expanded_features[:int(num_elements * .95)]\n",
            " 19: x_test = expanded_features[int(num_elements * .95):]\n",
            " 20: y_train = expanded_labels[:int(num_elements * .95)]\n",
            " 21: y_test = expanded_labels[int(num_elements* .95):]\n",
            " 22: \n",
            " 23: y_train = keras.utils.to_categorical(y_train, 10)\n",
            " 24: y_test = keras.utils.to_categorical(y_test, 10)\n",
            " 25: \n",
            " 26: \n",
            " 27: \n",
            " 28: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     \n",
            "   4:     \n",
            "   5:     #input layer\n",
            "   6:     model = keras.Sequential()\n",
            "   7: \n",
            "   8:     #convolution layer 1\n",
            "   9:     model.add(keras.layers.Conv2D(space['Conv2D'], space['Conv2D_1'], input_shape=(28, 28, 1), activation=space['activation']))\n",
            "  10: \n",
            "  11:     #pool layer 1\n",
            "  12:     #input [batch_size, 24, 24, 20]\n",
            "  13:     #output [batch_size, 12, 12, 20]\n",
            "  14:     model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
            "  15: \n",
            "  16:     #convolution layer 2\n",
            "  17:     model.add(keras.layers.Conv2D(space['Conv2D_2'], space['Conv2D_3'], activation=space['activation_1']))\n",
            "  18:    \n",
            "  19:     \n",
            "  20:     #pool layer 2\n",
            "  21:     #input [batch_size, 8, 8, 40]\n",
            "  22:     #output [batch_size, 4, 4, 40]\n",
            "  23:     model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
            "  24:     \n",
            "  25:     #pool layer 3\n",
            "  26:     if space['activation_2'] == 'add':\n",
            "  27: \n",
            "  28:         #convolution layer 2\n",
            "  29:         model.add(keras.layers.Conv2D(space['Conv2D_4'], (3,3), activation=space['activation_3']))\n",
            "  30:         model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
            "  31: \n",
            "  32:     model.add(keras.layers.Flatten())\n",
            "  33:     \n",
            "  34:     #dense layer 1\n",
            "  35:     model.add(keras.layers.Dense(space['Dense'], activation=space['activation_4']))\n",
            "  36:     \n",
            "  37:     #dropout layer\n",
            "  38:     model.add(keras.layers.Dropout(space['Dropout']))\n",
            "  39:     \n",
            "  40:     #dense layer 2\n",
            "  41:     model.add(keras.layers.Dense(space['Dense_1'], activation=space['activation_5']))\n",
            "  42:     \n",
            "  43:     #dropout layer\n",
            "  44:     model.add(keras.layers.Dropout(space['Dropout_1']))\n",
            "  45:     \n",
            "  46:     #output layer\n",
            "  47:     model.add(keras.layers.Dense(10, activation=tf.nn.softmax))       \n",
            "  48:     \n",
            "  49:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
            "  50:                   optimizer=space['optimizer'])\n",
            "  51: \n",
            "  52:     \n",
            "  53:     train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
            "  54:         rescale=1./255,\n",
            "  55:         rotation_range=30,\n",
            "  56:         width_shift_range=0.1,\n",
            "  57:         height_shift_range=0.1,\n",
            "  58:         zoom_range=0.1,\n",
            "  59:         fill_mode='constant',\n",
            "  60:         cval=0,\n",
            "  61:         horizontal_flip=True)\n",
            "  62: \n",
            "  63:     batch_size = 1\n",
            "  64:     \n",
            "  65:     if space['optimizer_1'] == 64:\n",
            "  66:         batch_size = 64\n",
            "  67:     else:\n",
            "  68:         batch_size = 128\n",
            "  69:     \n",
            "  70:     train_generator = train_datagen.flow(\n",
            "  71:         x_train, y_train,  \n",
            "  72:         batch_size=batch_size\n",
            "  73:     ) \n",
            "  74: \n",
            "  75: \n",
            "  76:     \n",
            "  77:     model.fit_generator(train_generator,\n",
            "  78:               steps_per_epoch=42000 // batch_size,\n",
            "  79:               epochs=1,\n",
            "  80:               verbose=2)\n",
            "  81:     score, acc = model.evaluate(x_test / 255, y_test, verbose=0)\n",
            "  82:     print('Test accuracy:', acc)\n",
            "  83:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
            "  84: \n",
            "Epoch 1/1\n",
            " - 19s - loss: 0.7408 - acc: 0.7466\n",
            "Test accuracy: 0.9304761903626578\n",
            "Epoch 1/1\n",
            " - 18s - loss: 1.8899 - acc: 0.2887\n",
            "Test accuracy: 0.7685714286849612\n",
            "Epoch 1/1\n",
            " - 15s - loss: 2.3228 - acc: 0.1157\n",
            "Test accuracy: 0.3757142856575194\n",
            "Epoch 1/1\n",
            " - 16s - loss: 1.0626 - acc: 0.6323\n",
            "Test accuracy: 0.8671428571428571\n",
            "Epoch 1/1\n",
            " - 16s - loss: 1.8172 - acc: 0.3472\n",
            "Test accuracy: 0.7680952378681728\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " - 18s - loss: 2.2114 - acc: 0.1755\n",
            "Test accuracy: 0.6133333333333333\n",
            "Epoch 1/1\n",
            " - 15s - loss: 2.3211 - acc: 0.1069\n",
            "Test accuracy: 0.22428571428571428\n",
            "Epoch 1/1\n",
            " - 17s - loss: 2.3040 - acc: 0.1091\n",
            "Test accuracy: 0.10142857144276302\n",
            "Epoch 1/1\n",
            " - 18s - loss: 2.1482 - acc: 0.1967\n",
            "Test accuracy: 0.5871428569157918\n",
            "Epoch 1/1\n",
            " - 16s - loss: 2.1954 - acc: 0.1698\n",
            "Test accuracy: 0.4376190473919823\n",
            "Epoch 1/1\n",
            " - 17s - loss: 1.8557 - acc: 0.3258\n",
            "Test accuracy: 0.7933333333333333\n",
            "Epoch 1/1\n",
            " - 16s - loss: 1.0360 - acc: 0.6446\n",
            "Test accuracy: 0.9061904761904762\n",
            "Epoch 1/1\n",
            " - 16s - loss: 0.9388 - acc: 0.6787\n",
            "Test accuracy: 0.9328571428571428\n",
            "Epoch 1/1\n",
            " - 19s - loss: 1.5014 - acc: 0.4637\n",
            "Test accuracy: 0.8352380951245626\n",
            "Epoch 1/1\n",
            " - 16s - loss: 1.3117 - acc: 0.5477\n",
            "Test accuracy: 0.876666666553134\n",
            "Epoch 1/1\n",
            " - 16s - loss: 1.2044 - acc: 0.5829\n",
            "Test accuracy: 0.884285714058649\n",
            "Epoch 1/1\n",
            " - 17s - loss: 1.3077 - acc: 0.5404\n",
            "Test accuracy: 0.857619047505515\n",
            "Epoch 1/1\n",
            " - 17s - loss: 1.0382 - acc: 0.6426\n",
            "Test accuracy: 0.8680952378681728\n",
            "Epoch 1/1\n",
            " - 18s - loss: 1.6520 - acc: 0.3925\n",
            "Test accuracy: 0.8204761903626578\n",
            "Epoch 1/1\n",
            " - 16s - loss: 2.3150 - acc: 0.1042\n",
            "Test accuracy: 0.10142857144276302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0BmsYBCsst_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1873
        },
        "outputId": "f67539e6-4b84-4f37-974f-7935bcc35062"
      },
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = data()\n",
        "\n",
        "print(best_run)\n",
        "\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=30,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        fill_mode='constant',\n",
        "        cval=0,\n",
        "        horizontal_flip=True)\n",
        "    \n",
        "\n",
        "batch_size = [64, 128][best_run['optimizer_1']]\n",
        "    \n",
        "train_generator = train_datagen.flow(\n",
        "        x_train, y_train,  \n",
        "        batch_size=batch_size\n",
        "    ) \n",
        "\n",
        "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "validation_generator = test_datagen.flow(\n",
        "        x_test, y_test,\n",
        "        batch_size=batch_size,\n",
        ")\n",
        "\n",
        "\n",
        "best_model.fit_generator(train_generator,\n",
        "          steps_per_epoch = 42000 // batch_size,\n",
        "          epochs=50,\n",
        "          verbose=2,\n",
        "          validation_data=validation_generator)\n",
        "predictions = best_model.predict(test_features)\n",
        "\n",
        "print(predictions)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Conv2D': 1, 'Conv2D_1': 1, 'Conv2D_2': 1, 'Conv2D_3': 1, 'Conv2D_4': 3, 'Dense': 3, 'Dense_1': 3, 'Dropout': 0.2855072647865474, 'Dropout_1': 0.5371229457196108, 'activation': 2, 'activation_1': 1, 'activation_2': 1, 'activation_3': 0, 'activation_4': 2, 'activation_5': 2, 'optimizer': 1, 'optimizer_1': 1}\n",
            "Epoch 1/50\n",
            " - 14s - loss: 0.4124 - acc: 0.8654 - val_loss: 0.1265 - val_acc: 0.9633\n",
            "Epoch 2/50\n",
            " - 14s - loss: 0.2985 - acc: 0.9054 - val_loss: 0.0893 - val_acc: 0.9729\n",
            "Epoch 3/50\n",
            " - 14s - loss: 0.2448 - acc: 0.9240 - val_loss: 0.0902 - val_acc: 0.9719\n",
            "Epoch 4/50\n",
            " - 14s - loss: 0.2126 - acc: 0.9346 - val_loss: 0.0821 - val_acc: 0.9752\n",
            "Epoch 5/50\n",
            " - 14s - loss: 0.1962 - acc: 0.9403 - val_loss: 0.0810 - val_acc: 0.9738\n",
            "Epoch 6/50\n",
            " - 14s - loss: 0.1817 - acc: 0.9444 - val_loss: 0.0887 - val_acc: 0.9710\n",
            "Epoch 7/50\n",
            " - 14s - loss: 0.1766 - acc: 0.9459 - val_loss: 0.0617 - val_acc: 0.9810\n",
            "Epoch 8/50\n",
            " - 14s - loss: 0.1594 - acc: 0.9512 - val_loss: 0.0745 - val_acc: 0.9800\n",
            "Epoch 9/50\n",
            " - 14s - loss: 0.1539 - acc: 0.9523 - val_loss: 0.0792 - val_acc: 0.9757\n",
            "Epoch 10/50\n",
            " - 14s - loss: 0.1509 - acc: 0.9539 - val_loss: 0.0709 - val_acc: 0.9776\n",
            "Epoch 11/50\n",
            " - 14s - loss: 0.1437 - acc: 0.9560 - val_loss: 0.0662 - val_acc: 0.9767\n",
            "Epoch 12/50\n",
            " - 14s - loss: 0.1427 - acc: 0.9571 - val_loss: 0.0704 - val_acc: 0.9795\n",
            "Epoch 13/50\n",
            " - 14s - loss: 0.1330 - acc: 0.9598 - val_loss: 0.0690 - val_acc: 0.9776\n",
            "Epoch 14/50\n",
            " - 14s - loss: 0.1374 - acc: 0.9583 - val_loss: 0.0591 - val_acc: 0.9824\n",
            "Epoch 15/50\n",
            " - 14s - loss: 0.1239 - acc: 0.9612 - val_loss: 0.0587 - val_acc: 0.9805\n",
            "Epoch 16/50\n",
            " - 14s - loss: 0.1260 - acc: 0.9611 - val_loss: 0.0650 - val_acc: 0.9824\n",
            "Epoch 17/50\n",
            " - 14s - loss: 0.1252 - acc: 0.9612 - val_loss: 0.0509 - val_acc: 0.9843\n",
            "Epoch 18/50\n",
            " - 14s - loss: 0.1242 - acc: 0.9629 - val_loss: 0.0959 - val_acc: 0.9710\n",
            "Epoch 19/50\n",
            " - 14s - loss: 0.1183 - acc: 0.9640 - val_loss: 0.0707 - val_acc: 0.9795\n",
            "Epoch 20/50\n",
            " - 14s - loss: 0.1205 - acc: 0.9632 - val_loss: 0.0634 - val_acc: 0.9810\n",
            "Epoch 21/50\n",
            " - 14s - loss: 0.1166 - acc: 0.9644 - val_loss: 0.0597 - val_acc: 0.9810\n",
            "Epoch 22/50\n",
            " - 14s - loss: 0.1142 - acc: 0.9649 - val_loss: 0.0584 - val_acc: 0.9814\n",
            "Epoch 23/50\n",
            " - 14s - loss: 0.1183 - acc: 0.9645 - val_loss: 0.0669 - val_acc: 0.9790\n",
            "Epoch 24/50\n",
            " - 14s - loss: 0.1132 - acc: 0.9647 - val_loss: 0.0684 - val_acc: 0.9767\n",
            "Epoch 25/50\n",
            " - 14s - loss: 0.1057 - acc: 0.9673 - val_loss: 0.0645 - val_acc: 0.9800\n",
            "Epoch 26/50\n",
            " - 14s - loss: 0.1101 - acc: 0.9664 - val_loss: 0.0761 - val_acc: 0.9767\n",
            "Epoch 27/50\n",
            " - 14s - loss: 0.1059 - acc: 0.9682 - val_loss: 0.0539 - val_acc: 0.9843\n",
            "Epoch 28/50\n",
            " - 14s - loss: 0.1087 - acc: 0.9663 - val_loss: 0.0675 - val_acc: 0.9795\n",
            "Epoch 29/50\n",
            " - 14s - loss: 0.1064 - acc: 0.9681 - val_loss: 0.0595 - val_acc: 0.9810\n",
            "Epoch 30/50\n",
            " - 14s - loss: 0.1039 - acc: 0.9677 - val_loss: 0.0466 - val_acc: 0.9857\n",
            "Epoch 31/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " - 14s - loss: 0.1096 - acc: 0.9668 - val_loss: 0.0504 - val_acc: 0.9843\n",
            "Epoch 32/50\n",
            " - 14s - loss: 0.1042 - acc: 0.9678 - val_loss: 0.0630 - val_acc: 0.9824\n",
            "Epoch 33/50\n",
            " - 14s - loss: 0.1064 - acc: 0.9678 - val_loss: 0.0546 - val_acc: 0.9843\n",
            "Epoch 34/50\n",
            " - 14s - loss: 0.1041 - acc: 0.9672 - val_loss: 0.0496 - val_acc: 0.9829\n",
            "Epoch 35/50\n",
            " - 14s - loss: 0.0994 - acc: 0.9696 - val_loss: 0.0517 - val_acc: 0.9810\n",
            "Epoch 36/50\n",
            " - 14s - loss: 0.0993 - acc: 0.9690 - val_loss: 0.0554 - val_acc: 0.9824\n",
            "Epoch 37/50\n",
            " - 14s - loss: 0.0961 - acc: 0.9711 - val_loss: 0.0566 - val_acc: 0.9800\n",
            "Epoch 38/50\n",
            " - 14s - loss: 0.1006 - acc: 0.9690 - val_loss: 0.0676 - val_acc: 0.9795\n",
            "Epoch 39/50\n",
            " - 14s - loss: 0.0985 - acc: 0.9700 - val_loss: 0.0607 - val_acc: 0.9843\n",
            "Epoch 40/50\n",
            " - 14s - loss: 0.1005 - acc: 0.9694 - val_loss: 0.0514 - val_acc: 0.9833\n",
            "Epoch 41/50\n",
            " - 14s - loss: 0.0963 - acc: 0.9709 - val_loss: 0.0479 - val_acc: 0.9857\n",
            "Epoch 42/50\n",
            " - 14s - loss: 0.0956 - acc: 0.9704 - val_loss: 0.0587 - val_acc: 0.9833\n",
            "Epoch 43/50\n",
            " - 14s - loss: 0.0930 - acc: 0.9708 - val_loss: 0.0479 - val_acc: 0.9862\n",
            "Epoch 44/50\n",
            " - 14s - loss: 0.0977 - acc: 0.9702 - val_loss: 0.0566 - val_acc: 0.9838\n",
            "Epoch 45/50\n",
            " - 14s - loss: 0.0963 - acc: 0.9702 - val_loss: 0.0554 - val_acc: 0.9829\n",
            "Epoch 46/50\n",
            " - 14s - loss: 0.0982 - acc: 0.9708 - val_loss: 0.0690 - val_acc: 0.9800\n",
            "Epoch 47/50\n",
            " - 14s - loss: 0.0920 - acc: 0.9718 - val_loss: 0.0582 - val_acc: 0.9833\n",
            "Epoch 48/50\n",
            " - 14s - loss: 0.0930 - acc: 0.9711 - val_loss: 0.0525 - val_acc: 0.9824\n",
            "Epoch 49/50\n",
            " - 14s - loss: 0.0911 - acc: 0.9719 - val_loss: 0.0695 - val_acc: 0.9805\n",
            "Epoch 50/50\n",
            " - 14s - loss: 0.0898 - acc: 0.9727 - val_loss: 0.0637 - val_acc: 0.9805\n",
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 1. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FPkTRAlLcD6w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "with open('submission.csv', 'w') as file:\n",
        "    file.write('ImageId,Label\\n')\n",
        "    for pred in predictions:\n",
        "        file.write(\"{},{}\\n\".format(i, np.argmax(pred)))\n",
        "        i += 1\n",
        "\n",
        "from google.colab import files\n",
        "files.download('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}